{"messages": [{"role": "user", "content": "What's the difference between unit tests, integration tests, and end-to-end tests?"}, {"role": "assistant", "content": "Unit tests: test single function/method in isolation, fast (milliseconds), mock all dependencies, example: test calculate_tax(100, 0.10) == 10. Integration tests: test multiple components together, slower (seconds), mock external systems only, example: test UserService with real database but mock email. End-to-end tests: test complete user flows through UI, slowest (minutes), minimal mocking, example: Selenium test of full login→dashboard→logout flow. Pyramid: many unit tests (70%), some integration (20%), few E2E (10%). Unit = precise errors, E2E = confidence in system. Use all three layers for robust test coverage."}]}
{"messages": [{"role": "user", "content": "When should I use mocks vs stubs vs spies?"}, {"role": "assistant", "content": "Mocks: verify behavior (was method called?), assert interactions, fail test if expectations not met. Use when: testing that code calls dependencies correctly. Stubs: provide canned responses, don't verify calls, just return fake data. Use when: need dependency to return specific value for test. Spies: record calls but use real implementation (partial mock), track interactions. Use when: want to verify calls while keeping real behavior. Example Mock: mock_api.get.assert_called_with(url). Example Stub: stub_db.get_user.return_value = fake_user. Example Spy: spy wraps real object, tracks all method calls. General rule: Stubs for state, Mocks for behavior verification, Spies when you need both. Prefer stubs for simpler tests."}]}
{"messages": [{"role": "user", "content": "What is Test-Driven Development and what are its benefits?"}, {"role": "assistant", "content": "TDD workflow: 1) Red - write failing test first, defines requirements, 2) Green - write minimal code to pass, focus on making it work, 3) Refactor - improve code while keeping tests green, improve design. Benefits: better design (testable code is well-designed), living documentation (tests show how code should work), regression safety (can refactor confidently), fewer bugs (caught early), forces small increments. Drawbacks: slower initial development, learning curve, can over-test. Best for: new features, bug fixes (write test that reproduces bug), complex logic. Not for: prototypes, throwaway code, UI layouts. Key: discipline to write test BEFORE code. TDD changes how you think about code - from 'how to build' to 'how to verify'."}]}
{"messages": [{"role": "user", "content": "How do I write good test assertions?"}, {"role": "assistant", "content": "Good assertion principles: 1) One logical assertion per test - test one thing, clear failure message, 2) Specific assertions - assert user.name == 'Alice' not assert len(user.name) > 0, 3) Descriptive messages - assert value == expected, f'{value} should equal {expected}', 4) Test behavior not implementation - assert login_successful not assert database.users.count() == 1, 5) Use appropriate matchers - toContain() for arrays, toBeCloseTo() for floats, 6) Avoid logic in tests - no if/for in assertions, 7) Assert expected state not just no error. Bad: assert no exception. Good: assert user.is_logged_in(). Frameworks: pytest (assert), Jest (expect().toBe()), JUnit (assertEquals()). Clear assertions = clear intent."}]}
{"messages": [{"role": "user", "content": "What's the Page Object Model pattern and why use it?"}, {"role": "assistant", "content": "Page Object Model (POM): design pattern for UI tests. Each page/component = class with locators + methods. LoginPage class has username_field, password_field, login(user, pass) method. Benefits: 1) Maintainability - locator changes in one place, 2) Readability - tests read like actions: page.login('user', 'pass'), 3) Reusability - methods used in multiple tests, 4) Separation - test logic separate from page structure. Without POM: driver.find_element(By.ID, 'user').send_keys('test') repeated everywhere. With POM: login_page.login('test') - cleaner, easier to maintain. Best practices: one class per page, locators as class constants, methods return other page objects for chaining: login_page.login().get_dashboard(). Use for: Selenium, Playwright, any UI automation. Essential for large test suites."}]}
{"messages": [{"role": "user", "content": "How should I organize my test files and fixtures?"}, {"role": "assistant", "content": "Test organization: 1) Mirror source structure - tests/unit/module_name_test.py matches src/module_name.py, 2) Group by feature - tests/features/login/, tests/features/checkout/, 3) Shared fixtures - conftest.py (pytest) or test_helpers.py, 4) Test data - tests/fixtures/data.json separate from test code, 5) Naming - test_*.py or *_test.py for discovery. Fixture organization: conftest.py for shared fixtures (auto-discovered), per-module fixtures in test file, fixture scope (function/class/module/session) determines lifecycle. Example structure: tests/unit/ (fast), tests/integration/ (moderate), tests/e2e/ (slow). Run subsets: pytest tests/unit. Best practice: tests easy to run, fast tests separate from slow, fixtures clear and reusable."}]}
{"messages": [{"role": "user", "content": "What's the difference between pytest and unittest?"}, {"role": "assistant", "content": "pytest: modern, simple assert statements, fixtures via decorators, parametrize for data-driven, auto-discovery, rich plugin ecosystem. Example: def test_x(): assert 1+1 == 2. unittest: standard library, class-based (inherit TestCase), setUp/tearDown methods, assertEqual() methods, verbose. Example: class TestMath(unittest.TestCase): def test_add(self): self.assertEqual(1+1, 2). pytest advantages: less boilerplate, better error messages, powerful fixtures, backwards compatible with unittest. unittest advantages: no dependencies, familiar to Java/JUnit users. Recommendation: pytest for new projects (more Pythonic, better features), unittest if avoiding dependencies. pytest can run unittest tests. Most Python projects use pytest now."}]}
{"messages": [{"role": "user", "content": "How do I test asynchronous code?"}, {"role": "assistant", "content": "Async testing strategies: pytest: use pytest-asyncio plugin, mark with @pytest.mark.asyncio, define async def test_async(), use await. Jest: test returns Promise or use async/await: test('async test', async () => { const result = await fetchData(); expect(result).toBe(value); }). Python: import asyncio; asyncio.run(async_function()) in tests. Mock async: async def mock_async(): return value; mock.side_effect = mock_async. Challenges: timing issues (use await not time.sleep), proper cleanup (ensure connections closed), flaky tests (timeouts). Tools: pytest-asyncio, pytest-trio, Jest built-in async. Best practice: test async functions with async tests, mock external async calls, use generous timeouts in CI, consider sync wrappers for complex async code in tests."}]}
{"messages": [{"role": "user", "content": "What are test fixtures and when should I use them?"}, {"role": "assistant", "content": "Fixtures: reusable setup code for tests, provide test data/resources, automatic cleanup. pytest fixtures: @pytest.fixture decorator, passed as function arguments, yield for cleanup. Uses: database connections (setup DB, yield connection, teardown), test data (create fake users/posts), mock objects, temporary files. Scopes: function (default, new per test), class (shared within class), module (shared in file), session (once per test run). Example: @pytest.fixture; def db(): conn = create_db(); yield conn; conn.close(). Benefits: DRY principle, consistent setup, automatic teardown, composable (fixtures can use other fixtures). When to use: setup is complex, state shared across tests, cleanup needed (files/connections). Avoid: fixtures that do too much, testing fixture logic itself."}]}
{"messages": [{"role": "user", "content": "How can I make my tests run faster?"}, {"role": "assistant", "content": "Speed optimization: 1) Run in parallel - pytest -n auto (pytest-xdist), Jest --maxWorkers=4, 2) Use test marks - run fast unit tests first: pytest -m unit, skip slow: -m 'not slow', 3) Mock expensive operations - don't hit real database/API, 4) Shared fixtures - session scope for expensive setup, 5) Optimize imports - lazy imports in tests, 6) Database: use in-memory DB (SQLite :memory:), rollback transactions instead of recreating, 7) Minimize I/O - mock file operations, 8) Profile tests - pytest --durations=10 shows slowest, 9) Test only changed code in development. Target: unit tests < 100ms each, full suite < 10min. Slow E2E tests in separate suite. CI: parallel jobs, cache dependencies. Fast tests = run often = catch bugs early."}]}
{"messages": [{"role": "user", "content": "What's code coverage and what percentage should I aim for?"}, {"role": "assistant", "content": "Code coverage: percentage of code executed by tests. Types: line coverage (lines run), branch coverage (if/else paths taken), function coverage. Tools: pytest --cov=module (coverage.py), Jest --coverage. Targets: 80%+ is good, 100% is unrealistic/unnecessary. Don't chase 100% - diminishing returns, some code hard to test (error handlers, edge cases). Focus: test important logic, business rules, bug-prone areas. Don't test: trivial getters/setters, framework code, generated code. Coverage ≠ quality - can have 100% coverage with bad tests. Better metric: mutation testing (does killing code fail tests?). Use coverage to find untested code, not as success metric. Green CI requires: 80% coverage, all tests pass, no flaky tests. Trend matters more than absolute number."}]}
{"messages": [{"role": "user", "content": "How do I write tests for code that uses databases?"}, {"role": "assistant", "content": "Database testing strategies: 1) In-memory DB - SQLite :memory:, fast, isolated, reset per test, 2) Test database - separate DB instance, real database engine, rollback transactions, 3) Docker containers - spin up DB in container, teardown after, 4) Mock database - mock ORM/query builder, fast but misses integration bugs. Pytest: use fixtures for DB setup: @pytest.fixture; def db(): create_db(); yield connection; drop_db(). Best practices: transactional tests (rollback after each), seed with factory_boy/faker, test at DAO layer not individual queries, separate unit (mock DB) from integration (real DB). Avoid: tests depending on order, shared state between tests, testing framework's DB code. For CI: use DB in docker-compose, fast enough for small datasets. Heavy data: use DB snapshots or synthetic data."}]}
{"messages": [{"role": "user", "content": "What are flaky tests and how do I fix them?"}, {"role": "assistant", "content": "Flaky tests: pass/fail inconsistently without code changes. Causes: 1) Timing - race conditions, insufficient waits, async issues, 2) Random data - random.random() in tests, UUIDs, 3) External dependencies - network calls, third-party APIs, 4) State pollution - tests share state, order-dependent, 5) Time-dependent - datetime.now() comparisons, 6) Resource limits - out of memory, port conflicts. Fixes: 1) Explicit waits - WebDriverWait, not time.sleep(), 2) Seed random - random.seed(42) for reproducibility, 3) Mock external - patch network calls, 4) Isolate tests - pytest-xdist, clean fixtures, 5) Freeze time - freezegun library, 6) Retry - pytest-rerunfailures as last resort. Detection: run tests 100 times, check CI history. Quarantine: mark @pytest.mark.flaky, fix separately. Zero tolerance for flakiness - kills confidence in test suite."}]}
{"messages": [{"role": "user", "content": "How should I structure test data and factories?"}, {"role": "assistant", "content": "Test data strategies: 1) Factory pattern - factory_boy (Python), factory functions: def create_user(name='Test', email='test@example.com'): return User(name, email). Advantages: reusable, customizable, valid defaults. 2) Fixtures - JSON/YAML files with sample data, load in tests. 3) Builders - fluent API: UserBuilder().with_name('Alice').with_email('alice@test.com').build(). 4) Object Mother - pre-defined test objects: TestUsers.ADMIN, TestUsers.REGULAR. Best practices: faker for realistic data (names, emails), avoid hardcoded IDs, make factories composable (create_order needs create_user), minimum data for test (only set required fields). Avoid: copying production data (privacy/size issues), magic values ('xxx', '123'), test data in test code (separate files). factory_boy example: UserFactory(name='Alice') creates user with random email/id, specified name."}]}
{"messages": [{"role": "user", "content": "What's the difference between black-box and white-box testing?"}, {"role": "assistant", "content": "Black-box testing: test functionality without knowing internal implementation, focus on inputs/outputs, examples: E2E tests, acceptance tests, API testing. Tester: writes tests based on requirements, doesn't see code. Advantages: tests user perspective, catches integration issues, resilient to refactoring. Disadvantages: can't test internal logic, may miss edge cases, hard to achieve coverage. White-box testing: test with knowledge of internals, test specific code paths/branches, examples: unit tests, code coverage, path testing. Tester: sees code, tests implementation details. Advantages: high coverage, tests edge cases, finds logic errors. Disadvantages: brittle (breaks on refactoring), can miss user-facing bugs. Best practice: both - unit tests (white-box) for logic, E2E tests (black-box) for flows. Integration tests: gray-box (some internal knowledge). Most projects: heavy white-box unit tests + light black-box E2E."}]}
{"messages": [{"role": "user", "content": "How do I test error handling and exceptions?"}, {"role": "assistant", "content": "Exception testing: pytest: with pytest.raises(ValueError): function_that_raises(). Jest: expect(() => throwError()).toThrow(). Assert message: pytest.raises(ValueError, match='invalid input'). Test patterns: 1) Test exception raised - verify error occurs for bad input, 2) Test exception message - assert error is descriptive, 3) Test exception type - right exception for error condition, 4) Test no exception - valid input doesn't raise. Example: def test_invalid_age(): with pytest.raises(ValueError, match='age must be positive'): create_user(age=-1). Also test: exception in async code (pytest.raises works with async), exception cleanup (resources released), exception logged (check log output), user-friendly error messages. Don't: catch all exceptions broadly, ignore exception testing. Error paths are code too - need tests. Often: error handling has bugs because it's rarely exercised in production."}]}
{"messages": [{"role": "user", "content": "What are property-based tests and when should I use them?"}, {"role": "assistant", "content": "Property-based testing: generate random inputs, verify properties hold. Library: Hypothesis (Python), fast-check (JS). Example: @given(st.integers()) def test_reverse_reverse(x): assert reverse(reverse(x)) == x tests reversing list is idempotent. Properties: idempotence, commutativity, invariants. Hypothesis generates 100+ random cases, finds edge cases you wouldn't think of. When to use: mathematical properties (sort is idempotent), parsers/encoders (encode→decode = identity), business rules (price always positive), finding edge cases. Benefits: finds weird bugs (empty lists, negative numbers, special characters), tests many scenarios with one test, shrinks failures to minimal case. Drawbacks: complex to write, slower than example-based, not for all code. Use: complement to example-based tests, especially for core logic. Start with examples, add properties for extra confidence."}]}
{"messages": [{"role": "user", "content": "How do I test code that makes HTTP requests?"}, {"role": "assistant", "content": "HTTP testing strategies: 1) Mock requests library - responses library (Python): @responses.activate; responses.add(responses.GET, 'url', json={'data': 'value'}), then requests.get() returns mock. 2) requests-mock - with requests_mock.Mocker() as m: m.get('url', text='response'). 3) VCR - record real HTTP, replay in tests, vcrpy library. 4) Mock at function level - patch function that makes request. 5) Test server - httpretty, Flask test client. Best: mock external APIs (fast, reliable), use real requests for own API (integration test). Pytest: @pytest.fixture; def mock_api(requests_mock): requests_mock.get('api/users', json=[...]). Test scenarios: success (200), errors (404, 500), timeout, network error, malformed JSON. Don't: make real HTTP calls in unit tests (slow, flaky, costs money). Exception: smoke tests in separate suite."}]}
{"messages": [{"role": "user", "content": "What's the arrange-act-assert pattern?"}, {"role": "assistant", "content": "AAA pattern: structure for readable tests. Arrange: set up test data/state. Act: execute the code under test. Assert: verify expected outcome. Example: def test_user_creation(): # Arrange; user_data = {'name': 'Alice', 'email': 'alice@test.com'}; # Act; user = create_user(user_data); # Assert; assert user.name == 'Alice'. Also called: Given-When-Then (BDD style). Benefits: clear structure, easy to read, separates setup/action/verification. Variants: for simple tests, arrange+act can combine. Avoid: multiple acts (split into separate tests), assertions in arrange (use fixtures), unclear boundaries. Comments optional if code is clear. Similar: Setup-Exercise-Verify-Teardown (with explicit cleanup). AAA works for all test types: unit, integration, E2E. Makes tests documentation of behavior."}]}
{"messages": [{"role": "user", "content": "How do I test private methods?"}, {"role": "assistant", "content": "Private method testing debate: Don't test private methods directly. Why: 1) Private = implementation detail, 2) Tests should test behavior not implementation, 3) Private methods tested via public interface, 4) Testing privates makes tests brittle. Instead: test public methods that call private methods, if private method complex, extract to separate class (now testable), treat private methods as refactoring detail. Exception: if language has no private (Python), convention is _private but still accessible, can test for coverage but risky. Bad: accessing private via reflection/tricks. Good: test public contract, refactor private to helper class if needed. If you want to test private method, question: should it be private? Complex private method = smell, maybe separate responsibility. Tests should survive refactoring - testing privates violates this. Focus: what not how."}]}
{"messages": [{"role": "user", "content": "What are integration tests and how are they different from unit tests?"}, {"role": "assistant", "content": "Integration tests: test multiple units working together, test interfaces between components, use some real dependencies (database, filesystem), slower than unit tests. Example: test UserService + real Database (not mock). Unit tests: test single function/class in isolation, mock all dependencies, fast (milliseconds), example: test calculate_tax function alone. Key differences: Scope (integration = multiple components, unit = one), Dependencies (integration = real or few mocks, unit = all mocked), Speed (integration = seconds, unit = milliseconds), Failures (integration = harder to debug, unit = pinpoint issue). When to use integration: verify components work together, test DAO layer with real DB, test API endpoints. Layers: unit → integration → E2E (increasing scope, decreasing speed). Pyramid: many unit (70%), some integration (20%), few E2E (10%). Both needed - unit for coverage, integration for confidence."}]}
{"messages": [{"role": "user", "content": "How should I handle test data cleanup?"}, {"role": "assistant", "content": "Cleanup strategies: 1) pytest fixtures with yield - @pytest.fixture; def resource(): obj = setup(); yield obj; cleanup(obj), automatic teardown, 2) unittest setUp/tearDown - run before/after each test, 3) try/finally - ensure cleanup even if test fails, 4) Database transactions - rollback after test, fast and clean, 5) Temporary files - tempfile.TemporaryDirectory() auto-deletes, 6) Docker containers - docker-compose down after suite. Best practices: clean up in same scope as setup, use context managers (with statements), fail test if cleanup fails, reset global state, clean up even on test failure. Pytest: autouse=True for automatic fixture application. Avoid: manual cleanup (error-prone), cleanup in next test's setup (fails if previous test skipped), leaving state for subsequent tests. Isolation: each test should be independent, runnable alone, in any order."}]}
{"messages": [{"role": "user", "content": "What's the difference between mocking, stubbing, and faking?"}, {"role": "assistant", "content": "Test doubles hierarchy: Dummy: passed but never used (fill parameter list). Stub: returns canned response, no verification, example: stub_db.get_user.return_value = User('test'). Spy: records calls, wraps real object, example: spy_logger tracks all log.info() calls. Mock: pre-programmed with expectations, verifies behavior, example: mock.method.assert_called_once_with(args). Fake: working implementation but simplified, example: in-memory database instead of PostgreSQL. When to use: Dummy for unused params, Stubs for providing test data, Spies for verifying calls while keeping behavior, Mocks for strict behavior verification, Fakes for complex dependencies (filesystem, database). Martin Fowler's terms. Most 'mocking libraries' provide all types. Prefer stubs over mocks (test state not behavior). Mocks = tightly coupled to implementation. Rule: mock types you own, fake types you don't (DB, HTTP)."}]}
{"messages": [{"role": "user", "content": "How do I test time-dependent code?"}, {"role": "assistant", "content": "Time testing strategies: 1) freezegun library - @freeze_time('2024-01-15 10:30:00'), datetime.now() returns frozen time, 2) Dependency injection - pass clock object, mock in tests, 3) timecop (Ruby style), 4) unittest.mock.patch - patch('datetime.datetime') but tricky. Problems with datetime.now(): tests fail at different times, can't reproduce bugs, flaky tests. Example: @freeze_time('2024-01-15'); def test_subscription_expired(): subscription.start_date = datetime(2024, 1, 1); subscription.end_date = datetime(2024, 1, 10); assert subscription.is_expired(). Benefits: tests deterministic, can test edge cases (midnight, leap years, timezones), fast (no actual waiting). Also: test time comparisons, expiration logic, date formatting, timezone handling. Avoid: time.sleep() in tests (use mocks), actual delays (waste time). Modern: prefer injecting clock to freezegun (explicit dependency)."}]}
{"messages": [{"role": "user", "content": "What are the benefits and drawbacks of TDD?"}, {"role": "assistant", "content": "TDD Benefits: 1) Better design - testable code is decoupled, 2) Living documentation - tests show intent, 3) Confidence - can refactor safely, 4) Fewer bugs - caught early when cheap to fix, 5) No over-engineering - only write needed code, 6) Regression suite - free from development. Drawbacks: 1) Slower initially - write test first takes time, 2) Learning curve - paradigm shift, 3) Not for everything - UI, exploratory work, 4) Can over-test - too many trivial tests, 5) Maintenance - tests need updating, 6) False security - tests can have bugs too. When TDD works: greenfield projects, complex algorithms, bug fixes (write test that fails, fix, test passes), teams with discipline. When skip: prototypes, spikes, simple CRUD, tight deadlines (technical debt). Key: test quality matters - bad tests worse than no tests. TDD is skill - improves with practice. Not religion - pragmatic use."}]}
{"messages": [{"role": "user", "content": "How do I write tests that are maintainable long-term?"}, {"role": "assistant", "content": "Maintainable test principles: 1) DRY - shared fixtures, helper functions, page objects, 2) Clear naming - test_should_return_error_when_user_not_found, 3) One assertion per test - clear failure, 4) Independent - tests don't depend on each other, 5) Fast - run frequently, 6) Minimal mocking - over-mocking = brittle, 7) Test behavior not implementation - survive refactoring, 8) Good error messages - assert with helpful descriptions, 9) Documented - complex tests have comments, 10) Avoid duplication - extract common setup. Patterns: AAA structure, Page Object Model, factory builders. Avoid: testing private methods, coupling to implementation, magic numbers (use constants), copy-paste tests, slow tests. Refactor tests like code - they're first-class citizens. Test code quality = production code quality. Bad tests = maintenance burden, eventually deleted, lose coverage. Good tests = development accelerator."}]}
{"messages": [{"role": "user", "content": "What's mutation testing and how is it useful?"}, {"role": "assistant", "content": "Mutation testing: modify (mutate) code, run tests, if tests still pass = insufficient testing. Example: change + to -, == to !=, if to if not. Tool introduces mutant, runs tests, mutant 'killed' if tests fail (good), 'survived' if tests pass (gap in testing). Tools: mutpy (Python), Stryker (JavaScript), PIT (Java). Metrics: mutation score = killed/total mutants. Example: change return x > 0 to return x >= 0, if tests pass, missing boundary test. Use: find weak tests, verify coverage quality, complement code coverage (100% line coverage ≠ good tests). Drawbacks: very slow (run full suite per mutant), many false positives (equivalent mutants), overwhelming results. When: critical code paths, after achieving high coverage, periodic quality check. Not: every commit (too slow). Mutation testing tests the tests - meta-testing. Expensive but reveals testing blind spots."}]}
{"messages": [{"role": "user", "content": "How do I test concurrent or multithreaded code?"}, {"role": "assistant", "content": "Concurrency testing challenges: race conditions, deadlocks, non-deterministic behavior. Strategies: 1) Stress testing - run many iterations, try to trigger race: for i in range(10000): concurrent_function(), 2) pytest-xdist - run tests in parallel to find issues, 3) Threading primitives - use locks/semaphores to coordinate test, 4) Mocking thread.sleep() - control timing, 5) Synchronization - barriers, events to control execution order. Tools: pytest-timeout (catch deadlocks), threading.Event for coordination. Example: def test_race_condition(): results = []; threads = [Thread(target=increment, args=(results,)) for _ in range(10)]; [t.start() for t in threads]; [t.join() for t in threads]; assert len(set(results)) == 10. Challenges: flaky tests, hard to reproduce, timeouts. Best: minimize concurrency (functional approach), use thread-safe structures, test with thread sanitizers. For critical code: formal verification, model checking. Most projects: test business logic separately from concurrency concerns."}]}
{"messages": [{"role": "user", "content": "What are snapshot tests and when should I use them?"}, {"role": "assistant", "content": "Snapshot testing: capture output, store as snapshot file, compare future runs. Popular in React (Jest snapshots). How: first run captures snapshot, subsequent runs compare to saved, fail if different. Example: expect(component).toMatchSnapshot(). Benefits: easy to write, catches unintended changes, good for stable outputs (HTML, config files). Drawbacks: 1) Large snapshots hard to review, 2) Developers blindly update snapshots without reviewing, 3) Not precise - changed anywhere = failed, 4) Snapshot files clutter repo. When to use: component rendering (React, Vue), API response structure (schema validation), generated code/configs. When not to use: business logic (use assertions), rapidly changing UI, dynamic data (timestamps, IDs). Best practices: small snapshots, review changes carefully, use inline snapshots for small outputs, combine with specific assertions. Alternative: schema validation (JSON Schema) more precise than snapshots. Snapshots = complement to traditional assertions, not replacement."}]}
{"messages": [{"role": "user", "content": "How do I set up continuous testing in CI/CD?"}, {"role": "assistant", "content": "CI/CD test integration: 1) GitHub Actions - .github/workflows/test.yml: 'on: [push, pull_request]; jobs: test: runs-on: ubuntu-latest; steps: - run: pytest', 2) GitLab CI - .gitlab-ci.yml: 'test: script: pytest', 3) Jenkins - Jenkinsfile: 'stage(\"Test\") { steps { sh \"pytest\" } }'. Best practices: run tests on every commit/PR, fail build if tests fail, parallel test execution (pytest -n auto), test reports (JUnit XML), coverage reports (Codecov, Coveralls), different stages (fast unit tests → slower integration → E2E), matrix testing (multiple Python/Node versions), cache dependencies (pip cache, npm cache), test in Docker for consistency, alerts on failure (Slack, email). Example workflow: PR created → run unit tests (2min) → run integration (5min) → run E2E on main branch only (15min). Optimize: only run affected tests, run fast tests first, fail fast (-x flag). Goal: tests complete in < 10 minutes for good dev experience."}]}
