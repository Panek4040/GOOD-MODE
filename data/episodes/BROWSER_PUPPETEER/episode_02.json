{
  "title": "Advanced Web Scraping with Network Interception",
  "description": "Scrape single-page application with API interception and data extraction",
  "difficulty": "advanced",
  "estimated_time": "65 minutes",
  "tools_used": [
    "puppeteer",
    "request_interception",
    "api_monitoring"
  ],
  "attack_chain": [
    {
      "step": 1,
      "title": "Initialize with Network Monitoring",
      "command": "const puppeteer = require('puppeteer'); const browser = await puppeteer.launch({headless: true}); const page = await browser.newPage(); const apiResponses = []; page.on('response', async response => { if (response.url().includes('/api/')) { apiResponses.push({url: response.url(), status: response.status(), data: await response.json().catch(() => null)}); } });",
      "explanation": "Capture all API calls made by SPA",
      "expected_output": "Response listener active",
      "success_criteria": "Network monitoring enabled"
    },
    {
      "step": 2,
      "title": "Block Unnecessary Resources",
      "command": "await page.setRequestInterception(true); page.on('request', request => { const blockedTypes = ['image', 'stylesheet', 'font', 'media']; if (blockedTypes.includes(request.resourceType())) { request.abort(); } else { request.continue(); } });",
      "explanation": "Speed up scraping by blocking media",
      "expected_output": "Resource blocking active",
      "success_criteria": "Faster page loads"
    },
    {
      "step": 3,
      "title": "Navigate to Target SPA",
      "command": "await page.goto('https://jsonplaceholder.typicode.com/', {waitUntil: 'networkidle0'}); console.log('SPA loaded');",
      "explanation": "Load single-page application",
      "expected_output": "SPA loaded",
      "success_criteria": "Page ready"
    },
    {
      "step": 4,
      "title": "Trigger Dynamic Content Loading",
      "command": "await page.evaluate(() => { fetch('/posts').then(r => r.json()).then(d => console.log('Posts:', d.length)); fetch('/users').then(r => r.json()).then(d => console.log('Users:', d.length)); }); await page.waitForTimeout(3000);",
      "explanation": "Execute AJAX requests via page context",
      "expected_output": "API calls triggered",
      "success_criteria": "Data loading initiated"
    },
    {
      "step": 5,
      "title": "Extract Intercepted API Data",
      "command": "console.log('Intercepted API calls:', apiResponses.length); const posts = apiResponses.find(r => r.url.includes('/posts'))?.data; const users = apiResponses.find(r => r.url.includes('/users'))?.data; console.log('Posts:', posts?.length, 'Users:', users?.length);",
      "explanation": "Parse data from intercepted network traffic",
      "expected_output": "Posts: 100 Users: 10",
      "success_criteria": "API data captured"
    },
    {
      "step": 6,
      "title": "Infinite Scroll Simulation",
      "command": "for (let i = 0; i < 5; i++) { await page.evaluate(() => window.scrollTo(0, document.body.scrollHeight)); await page.waitForTimeout(1000); } const scrollHeight = await page.evaluate(() => document.body.scrollHeight); console.log('Scrolled to:', scrollHeight);",
      "explanation": "Load more content via scrolling",
      "expected_output": "Scrolled to: 8000+",
      "success_criteria": "Additional content loaded"
    },
    {
      "step": 7,
      "title": "Extract DOM Data After Load",
      "command": "const domData = await page.evaluate(() => { const items = Array.from(document.querySelectorAll('article, .post, .item')); return items.map(item => ({text: item.textContent.trim().substring(0, 100), links: Array.from(item.querySelectorAll('a')).map(a => a.href)})); }); console.log('DOM items:', domData.length);",
      "explanation": "Scrape dynamically loaded elements",
      "expected_output": "DOM items: 50+",
      "success_criteria": "Data extracted"
    },
    {
      "step": 8,
      "title": "Modify Request Headers",
      "command": "page.removeAllListeners('request'); await page.setRequestInterception(true); page.on('request', request => { const headers = Object.assign({}, request.headers(), {'X-Custom-Header': 'PuppeteerBot', 'Authorization': 'Bearer fake-token'}); request.continue({headers}); }); await page.reload({waitUntil: 'networkidle0'});",
      "explanation": "Inject custom headers into requests",
      "expected_output": "Headers modified",
      "success_criteria": "Custom headers sent"
    },
    {
      "step": 9,
      "title": "Monitor Performance Metrics",
      "command": "const metrics = await page.metrics(); const performance = await page.evaluate(() => JSON.stringify(performance.timing)); console.log('Metrics:', {JSHeapUsedSize: (metrics.JSHeapUsedSize / 1048576).toFixed(2) + 'MB', Nodes: metrics.Nodes, LayoutCount: metrics.LayoutCount}); console.log('Timing:', JSON.parse(performance).domContentLoadedEventEnd - JSON.parse(performance).navigationStart, 'ms');",
      "explanation": "Collect page performance data",
      "expected_output": "Load time: 500-2000ms",
      "success_criteria": "Metrics captured"
    },
    {
      "step": 10,
      "title": "Export All Collected Data",
      "command": "const fs = require('fs'); fs.writeFileSync('/tmp/scraped_data.json', JSON.stringify({api: apiResponses, dom: domData, metrics: await page.metrics()}, null, 2)); console.log('Data exported to /tmp/scraped_data.json'); await browser.close();",
      "explanation": "Save all scraped data to file",
      "expected_output": "Data exported",
      "success_criteria": "File created"
    }
  ],
  "learning_objectives": [
    "Request interception",
    "API monitoring",
    "SPA scraping",
    "Performance analysis",
    "Header manipulation",
    "Resource optimization"
  ],
  "mitigation": [
    "API rate limiting",
    "Request signature validation",
    "Origin checking",
    "User-agent filtering"
  ],
  "tags": [
    "puppeteer",
    "scraping",
    "api_interception",
    "spa",
    "performance"
  ]
}
