{"instruction": "Find all files larger than 100MB in /var", "input": {"path": "/var", "min_size_mb": "100"}, "output": {"tool": "find_files_above_size", "action": "find /var -type f -size +100M", "result": "/var/log/syslog.1\n/var/lib/mysql/ibdata1\n/var/cache/apt/archives/large_package.deb"}}
{"instruction": "Show the 10 largest files in /home", "input": {"path": "/home", "limit": "10"}, "output": {"tool": "find_top_n_largest", "action": "bash -c \"find /home -type f -printf '%s|%p\\n' | sort -rn | head -n 10\"", "result": "2147483648|/home/user/backup.tar.gz\n1073741824|/home/user/videos/movie.mkv\n536870912|/home/user/database.sql"}}
{"instruction": "Find all large log files over 50MB in /var/log", "input": {"path": "/var/log", "extension": "log", "min_size_mb": "50"}, "output": {"tool": "find_large_by_extension", "action": "bash -c \"find /var/log -type f -name '*.log' -size +50M\"", "result": "/var/log/syslog.log\n/var/log/apache2/access.log\n/var/log/mysql/error.log"}}
{"instruction": "Find files over 200MB that are older than 30 days", "input": {"path": "/var/backups", "min_size_mb": "200", "days_old": "30"}, "output": {"tool": "find_large_with_age", "action": "find /var/backups -type f -size +200M -mtime +30", "result": "/var/backups/archive_october.tar.gz\n/var/backups/full_backup_sept.tar"}}
{"instruction": "Show which directories contain the most large files over 100MB", "input": {"path": "/", "min_size_mb": "100"}, "output": {"tool": "find_large_group_by_dir", "action": "bash -c \"find / -type f -size +100M -printf '%h\\n' | sort | uniq -c | sort -rn\"", "result": "     23 /var/lib/mysql\n     15 /var/log\n      8 /home/user/videos"}}
{"instruction": "Find large files and show who owns them", "input": {"path": "/opt", "min_size_mb": "50"}, "output": {"tool": "find_large_with_owner", "action": "bash -c \"find /opt -type f -size +50M -printf '%s|%u|%g|%p\\n' | sort -rn\"", "result": "524288000|app|app|/opt/app/data.db\n314572800|root|root|/opt/backups/archive.tar.gz"}}
{"instruction": "Generate statistics for all files over 100MB in /var/lib", "input": {"path": "/var/lib", "min_size_mb": "100", "limit": "20"}, "output": {"tool": "find_large_summary_stats", "action": "bash -c \"find /var/lib -type f -size +100M | wc -l\"", "result": "45"}}
{"instruction": "List the top 20 largest files in /usr", "input": {"path": "/usr", "limit": "20"}, "output": {"tool": "find_top_n_largest", "action": "bash -c \"find /usr -type f -printf '%s|%p|%TY-%Tm-%Td\\n' | sort -rn | head -n 20\"", "result": "52428800|/usr/lib/libreoffice.so|2025-10-15\n41943040|/usr/share/icons/theme.tar|2025-10-12"}}
{"instruction": "Find PDF files larger than 10MB", "input": {"path": "/home/user/documents", "extension": "pdf", "min_size_mb": "10"}, "output": {"tool": "find_large_by_extension", "action": "bash -c \"find /home/user/documents -type f -name '*.pdf' -size +10M -printf '%s|%p|%TY-%Tm-%Td\\n' | sort -rn\"", "result": "52428800|/home/user/documents/manual.pdf|2025-11-10\n31457280|/home/user/documents/guide.pdf|2025-11-08"}}
{"instruction": "Find database files over 500MB older than 14 days", "input": {"path": "/var/lib", "min_size_mb": "500", "days_old": "14"}, "output": {"tool": "find_large_with_age", "action": "bash -c \"find /var/lib -type f -size +500M -mtime +14 -printf '%s|%p|%TY-%Tm-%Td\\n' | sort -rn\"", "result": "2147483648|/var/lib/mysql/ibdata1|2025-10-25\n1073741824|/var/lib/postgresql/data.db|2025-10-28"}}
{"instruction": "Show total size of large files per directory", "input": {"path": "/srv", "min_size_mb": "100"}, "output": {"tool": "find_large_group_by_dir", "action": "bash -c \"find /srv -type f -size +100M -printf '%h|%s\\n' | awk -F'|' '{dir=\\$1; size=\\$2; sum[dir]+=size} END {for (d in sum) print sum[d]\\\"|\\\"d}' | sort -rn\"", "result": "5368709120|/srv/data\n2147483648|/srv/backups"}}
{"instruction": "Find which users own the largest files", "input": {"path": "/home", "min_size_mb": "100"}, "output": {"tool": "find_large_with_owner", "action": "bash -c \"find /home -type f -size +100M -printf '%u\\n' | sort | uniq -c | sort -rn\"", "result": "     15 alice\n      8 bob\n      3 carol"}}
{"instruction": "Get statistics on files over 50MB", "input": {"path": "/opt", "min_size_mb": "50", "limit": "15"}, "output": {"tool": "find_large_summary_stats", "action": "bash -c \"find /opt -type f -size +50M -printf '%s\\n' | awk '{sum+=\\$1; if(NR==1){min=max=\\$1} if(\\$1<min){min=\\$1} if(\\$1>max){max=\\$1}} END {print \\\"Total: \\\"sum; print \\\"Avg: \\\"sum/NR}'\"", "result": "Total: 10737418240\nAvg: 536870912"}}
{"instruction": "Find the 5 biggest files in /var/www", "input": {"path": "/var/www", "limit": "5"}, "output": {"tool": "find_top_n_largest", "action": "bash -c \"find /var/www -type f -printf '%s %p\\n' | sort -rn | head -n 5 | numfmt --to=iec --field=1\"", "result": "    250M /var/www/html/videos/demo.mp4\n    128M /var/www/backups/site.tar.gz"}}
{"instruction": "Find compressed archives over 100MB", "input": {"path": "/var/backups", "extension": "gz", "min_size_mb": "100"}, "output": {"tool": "find_large_by_extension", "action": "bash -c \"find /var/backups -type f -name '*.gz' -size +100M\"", "result": "/var/backups/full_2025.tar.gz\n/var/backups/incremental.tar.gz"}}
{"instruction": "Find video files larger than 500MB that haven't been accessed in 60 days", "input": {"path": "/home/user/videos", "min_size_mb": "500", "days_old": "60"}, "output": {"tool": "find_large_with_age", "action": "find /home/user/videos -type f -size +500M -atime +60", "result": "/home/user/videos/old_recording.mkv\n/home/user/videos/archive_2023.mp4"}}
{"instruction": "Count large files per directory in /opt", "input": {"path": "/opt", "min_size_mb": "200"}, "output": {"tool": "find_large_group_by_dir", "action": "bash -c \"find /opt -type f -size +200M -exec dirname {} \\; | sort | uniq -c\"", "result": "      5 /opt/app/data\n      3 /opt/backups\n      2 /opt/databases"}}
{"instruction": "Show large files grouped by owner with total sizes", "input": {"path": "/var/lib", "min_size_mb": "100"}, "output": {"tool": "find_large_with_owner", "action": "bash -c \"find /var/lib -type f -size +100M -printf '%u|%s\\n' | awk -F'|' '{owner=\\$1; size=\\$2; sum[owner]+=size; count[owner]++} END {for (o in sum) print sum[o]\\\"|\\\"count[o]\\\"|\\\"o}' | sort -rn\"", "result": "10737418240|12|mysql\n5368709120|5|postgres"}}
{"instruction": "Calculate min, max, and average size for files over 200MB", "input": {"path": "/var", "min_size_mb": "200", "limit": "10"}, "output": {"tool": "find_large_summary_stats", "action": "bash -c \"find /var -type f -size +200M -printf '%s\\n' | awk '{sum+=\\$1; if(NR==1){min=max=\\$1} if(\\$1<min){min=\\$1} if(\\$1>max){max=\\$1}} END {print \\\"Min: \\\"min; print \\\"Max: \\\"max; print \\\"Avg: \\\"sum/NR}'\"", "result": "Min: 209715200\nMax: 5368709120\nAvg: 1073741824"}}
{"instruction": "Find top 15 largest files with human-readable sizes", "input": {"path": "/srv", "limit": "15"}, "output": {"tool": "find_top_n_largest", "action": "bash -c \"find /srv -type f -exec du -b {} + | sort -rn | head -n 15\"", "result": "2147483648\t/srv/data/main.db\n1073741824\t/srv/backups/archive.tar"}}
{"instruction": "Find all SQL files over 100MB", "input": {"path": "/var/lib", "extension": "sql", "min_size_mb": "100"}, "output": {"tool": "find_large_by_extension", "action": "bash -c \"find /var/lib -type f -name '*.sql' -size +100M -exec ls -lh {} \\;\"", "result": "-rw-r--r-- 1 mysql mysql 250M Nov 15 10:30 /var/lib/mysql/dump.sql\n-rw-r--r-- 1 postgres postgres 180M Nov 14 14:22 /var/lib/postgresql/export.sql"}}
{"instruction": "Find large tar archives not modified in 90 days", "input": {"path": "/var/backups", "min_size_mb": "300", "days_old": "90"}, "output": {"tool": "find_large_with_age", "action": "bash -c \"find /var/backups -type f -size +300M -mtime +90 -exec stat -c '%s|%n|%y' {} \\;\"", "result": "524288000|/var/backups/Q2_2025.tar|2025-08-15 10:30:00.123456 +0000\n419430400|/var/backups/old_system.tar|2025-07-20 14:22:00.987654 +0000"}}
{"instruction": "List directories sorted by count of large files", "input": {"path": "/home", "min_size_mb": "50"}, "output": {"tool": "find_large_group_by_dir", "action": "bash -c \"for dir in \\$(find /home -type f -size +50M -printf '%h\\n' | sort -u); do echo \\\"\\$dir:\\\"; find \\\"\\$dir\\\" -maxdepth 1 -type f -size +50M -printf '  %s|%f\\n'; done\"", "result": "/home/alice/videos:\n  524288000|movie1.mkv\n  419430400|movie2.mp4\n/home/bob/backups:\n  1073741824|full.tar.gz"}}
{"instruction": "Show cumulative size owned by each user for files over 100MB", "input": {"path": "/opt", "min_size_mb": "100"}, "output": {"tool": "find_large_with_owner", "action": "bash -c \"find /opt -type f -size +100M -exec stat -c '%s|%U|%G|%n' {} \\; | sort -rn\"", "result": "2147483648|app|app|/opt/app/database.db\n536870912|root|root|/opt/backups/system.tar"}}
{"instruction": "Count total number of files over 250MB", "input": {"path": "/var/lib", "min_size_mb": "250"}, "output": {"tool": "find_large_summary_stats", "action": "bash -c \"find /var/lib -type f -size +250M | wc -l\"", "result": "28"}}
{"instruction": "Show 25 largest files with timestamps", "input": {"path": "/", "limit": "25"}, "output": {"tool": "find_top_n_largest", "action": "bash -c \"find / -type f -printf '%s|%p|%TY-%Tm-%Td %TH:%TM\\n' 2>/dev/null | sort -rn | head -n 25\"", "result": "5368709120|/var/lib/mysql/ibdata1|2025-11-18 14:30\n2147483648|/home/user/backup.tar.gz|2025-11-17 10:22"}}
{"instruction": "Find ISO images over 1GB", "input": {"path": "/opt/images", "extension": "iso", "min_size_mb": "1000"}, "output": {"tool": "find_large_by_extension", "action": "bash -c \"find /opt/images -type f -name '*.iso' -size +1000M -printf '%s|%p\\n' | sort -rn\"", "result": "4700372992|/opt/images/ubuntu-22.04.iso|2025-10-15\n3758096384|/opt/images/debian-12.iso|2025-09-28"}}
{"instruction": "Find backup files over 200MB not accessed in 180 days", "input": {"path": "/mnt/archives", "min_size_mb": "200", "days_old": "180"}, "output": {"tool": "find_large_with_age", "action": "bash -c \"find /mnt/archives -type f -size +200M -atime +180 -printf '%s|%p|%AY-%Am-%Ad\\n'\"", "result": "1073741824|/mnt/archives/2024_Q1.tar.gz|2024-05-15\n838860800|/mnt/archives/legacy.zip|2024-03-20"}}
{"instruction": "Show directory-wise breakdown of large file sizes", "input": {"path": "/var", "min_size_mb": "100"}, "output": {"tool": "find_large_group_by_dir", "action": "bash -c \"find /var -type f -size +100M -printf '%h|%s\\n' | awk -F'|' '{dir=\\$1; size=\\$2; sum[dir]+=size; count[dir]++} END {for (d in sum) print sum[d]\\\"|\\\"count[d]\\\"|\\\"d}' | sort -rn\"", "result": "10737418240|12|/var/lib/mysql\n5368709120|8|/var/log\n2147483648|3|/var/cache"}}
{"instruction": "List users with most large files and total size", "input": {"path": "/home", "min_size_mb": "200"}, "output": {"tool": "find_large_with_owner", "action": "bash -c \"find /home -type f -size +200M -printf '%u|%s\\n' | awk -F'|' '{sum[\\$1]+=\\$2; cnt[\\$1]++} END {for(u in sum) print sum[u]\\\"|\\\"cnt[u]\\\"|\\\"u}' | sort -rn\"", "result": "5368709120|5|alice\n2147483648|3|bob"}}
{"instruction": "Generate distribution of file sizes over 100MB", "input": {"path": "/opt", "min_size_mb": "100", "limit": "50"}, "output": {"tool": "find_large_summary_stats", "action": "bash -c \"find /opt -type f -size +100M -printf '%s\\n' | numfmt --to=iec | sort | uniq -c\"", "result": "     15 100M\n      8 200M\n      5 500M\n      3 1.0G\n      2 2.0G"}}
{"instruction": "Find the 30 largest files system-wide", "input": {"path": "/", "limit": "30"}, "output": {"tool": "find_top_n_largest", "action": "bash -c \"find / -type f -printf '%s|%p\\n' 2>/dev/null | sort -rn | head -n 30\"", "result": "10737418240|/var/lib/mysql/ibdata1\n5368709120|/home/user/large_video.mkv"}}
{"instruction": "Find MP4 videos larger than 200MB", "input": {"path": "/home", "extension": "mp4", "min_size_mb": "200"}, "output": {"tool": "find_large_by_extension", "action": "bash -c \"find /home -type f -name '*.mp4' -size +200M -printf '%s|%p|%TY-%Tm-%Td\\n' | sort -rn\"", "result": "1073741824|/home/user/videos/conference.mp4|2025-11-10\n524288000|/home/alice/recordings/presentation.mp4|2025-11-08"}}
{"instruction": "Find files over 1GB not modified in past year", "input": {"path": "/var/archives", "min_size_mb": "1000", "days_old": "365"}, "output": {"tool": "find_large_with_age", "action": "find /var/archives -type f -size +1000M -mtime +365", "result": "/var/archives/2023_backup.tar.gz\n/var/archives/old_data.zip"}}
